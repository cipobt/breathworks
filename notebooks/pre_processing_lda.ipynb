{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonny\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jonny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_columns.xlsx',\n",
       " 'data columns.xlsx',\n",
       " 'omfh_backup.csv',\n",
       " 'pre_processing.ipynb',\n",
       " 'pre_processing_lda.ipynb',\n",
       " 'textual_data.csv']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omfh_backup.csv', 'textual_data.csv']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [f for f in os.listdir(os.path.join(os.getcwd())) if f.endswith('.csv')]\n",
    "\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What condition(s) do you have that has brought you to this course?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety\\r\\nEDS - Hypermobility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronic fatigue\\r\\nDepression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fibromyalgie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chronic pain / muscular tension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>Stress of work, trouble to find deep sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>I am a psychotherapist, already using mindfuln...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>Stress and Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>Work-related and general stress, anxiety and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>dsa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2639 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     What condition(s) do you have that has brought you to this course?\n",
       "0                        Anxiety\\r\\nEDS - Hypermobility                \n",
       "1                        Chronic fatigue\\r\\nDepression                 \n",
       "2                                          Fibromyalgie                \n",
       "3                                                   NaN                \n",
       "4                       Chronic pain / muscular tension                \n",
       "...                                                 ...                \n",
       "2634       Stress of work, trouble to find deep sleep.                 \n",
       "2635  I am a psychotherapist, already using mindfuln...                \n",
       "2636                               Stress and Anxiety                  \n",
       "2637  Work-related and general stress, anxiety and r...                \n",
       "2638                                                dsa                \n",
       "\n",
       "[2639 rows x 1 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for f in file_paths:\n",
    "    dfs.append(pd.read_csv(os.path.join(os.getcwd(),f), encoding='ISO-8859-1'))\n",
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual = ['Motivation',\n",
    "       'Are you able to practice mindfulness exercises/techniques for at least 20 minutes per day for the duration of the course?',\n",
    "       'Which of these have you done with Breathworks so far? (tick any that apply)',\n",
    "       'If you have particular access or communication requirements which need to be met in order for you to engage fully with this online retreat, please let us know.',\n",
    "       'Is someone else paying for your course (not including employer)?',\n",
    "       'How did you hear about Breathworks?', 'PersonalHistory']\n",
    "onehote = ['CourseType', 'Gender', 'Ethnicity', 'Is your organisation paying for the course?',\n",
    "       'City', 'County']\n",
    "date = ['CourseDate', 'EnrollmentDate',\n",
    "       'DoB']\n",
    "columns_to_drop = ['No Label', 'No Label.1', 'If Yes, please provide the name of a person that appears on a receipt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs[0].copy().drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_convert = textual\n",
    "# df = df[columns_to_convert].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ')  # Remove Punctuation\n",
    "    lowercased = text.lower()  # Lower Case\n",
    "    tokenized = word_tokenize(lowercased)  # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()]  # Remove numbers\n",
    "\n",
    "    # Make stopword list and add 'yes' to it\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['yes','none'])  # Add 'yes' to the set of stopwords\n",
    "\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words]  # Remove Stop Words\n",
    "    lemma = WordNetLemmatizer()  # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords]  # Lemmatize\n",
    "    cleaned = ' '.join(lemmatized)  # Join back to a string\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in textual:\n",
    "#     df[f'clean_{column}'] = df[column].apply(clean)\n",
    "\n",
    "# df_c = df.drop(columns=textual)\n",
    "# df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dfs[1]\n",
    "df_test.rename(columns={'What condition(s) do you have that has brought you to this course?': 'brought'}, inplace=True)\n",
    "\n",
    "df_test['brought'] = df_test['brought'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df_test['brought'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(cleaned)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=3)\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('stress', 340.7284545684226), ('work', 304.35815629000507), ('mindfulness', 272.21237970680636), ('course', 165.8465557750626), ('life', 162.04792453351828), ('like', 154.1891983801961), ('would', 138.98741151275988), ('help', 132.63060502544002), ('year', 114.987446191888), ('practice', 106.94920179967892)]\n",
      "Topic 1:\n",
      "[('pain', 551.9993584887495), ('chronic', 326.9561326018829), ('back', 189.9650790546447), ('year', 156.49326750760406), ('anxiety', 137.61635027374703), ('fatigue', 137.31860342279813), ('fibromyalgia', 132.32601779453034), ('syndrome', 113.32545614833009), ('condition', 106.87139123199437), ('depression', 102.64552653444089)]\n",
      "Topic 2:\n",
      "[('mindfulness', 881.4401714831554), ('course', 666.7210991771921), ('would', 425.6623510667619), ('year', 360.5192863004976), ('like', 359.42254736501076), ('practice', 352.70817803968816), ('teacher', 288.214786898242), ('meditation', 280.82736128140635), ('stress', 262.22995847005734), ('training', 259.8755874772917)]\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\"I am looking for more help with my a connection between my mind and body, and maybe for someone to teach me ways to stay more calm and mindfull, i don't suffer from that much physical pain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : 0.027200785586826714\n",
      "topic 1 : 0.18504450123726074\n",
      "topic 1 : 0.7877547131759126\n"
     ]
    }
   ],
   "source": [
    "example_vectorized = vectorizer.transform(example)\n",
    "\n",
    "lda_vectors = lda_model.transform(example_vectorized)\n",
    "\n",
    "print(\"topic 0 :\", lda_vectors[0][0])\n",
    "print(\"topic 1 :\", lda_vectors[0][1])\n",
    "print(\"topic 1 :\", lda_vectors[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, func):\n",
    "#         self.func = func\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         # Ensure X is a DataFrame\n",
    "#         if isinstance(X, pd.Series):\n",
    "#             X = X.to_frame()\n",
    "#         return X.applymap(self.func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Applying the custom text preprocessing function to the textual columns\n",
    "# text_preprocessing_pipeline = Pipeline(steps=[\n",
    "#     ('text_preprocessor', TextPreprocessor(preprocessing)),\n",
    "#     ('tfidf', TfidfVectorizer())\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = ColumnTransformer(transformers=[\n",
    "#     ('text', text_preprocessing_pipeline, textual),\n",
    "#     ('onehot', OneHotEncoder(), onehote),\n",
    "#     # Add date processing as needed\n",
    "# ], remainder='passthrough')\n",
    "\n",
    "# # Apply the preprocessing\n",
    "# # You'll need to convert the output back to a DataFrame if you want to keep using it as such, since ColumnTransformer returns an array\n",
    "# df_preprocessed = preprocessor.fit_transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
